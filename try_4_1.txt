###############################################################################
#####  Raspberry Pi 4 Cluster Build [T4]
###############################################################################
##### Using code based on the following sites: 
##### https://limpygnome.com/2019/09/21/raspberry-pi-kubernetes-cluster/
#####
##### https://github.com/teamserverless/k8s-on-raspbian/blob/master/GUIDE.md
#####
##### https://blog.alexellis.io/serverless-kubernetes-on-raspberry-pi
##############################################################################

###############################################################################
##### Item 1 - Flash microSD cards with Linus OS
###############################################################################
Item 1.1 - Use Mac with etcher to load Raspbian Lite on microSD card.
Item 1.2 - 2019-09-26-raspbian-buster-lite.img
Item 1.3 - Create file named ssh in Volumes/boot
Item 1.3.1 - touch Volumes/boot/ssh

###############################################################################
##### Item 2 - Four microSD Cards, One Master And Three Workers
###############################################################################

###############################################################################
##### Item 3 - Insert microSD Cards And Power Up Network
###############################################################################
Item 3.1 - Use an Ethernet switch to connect the compute nodes
Item 3.2 - Use a different Linux or Mac computer to ssh into the node
Item 3.3 - Use namp -sP 192.168.0.0/24 get local LAN ip addresses

###############################################################################
##### Item 4 - Configure Base Master Node - m1
###############################################################################
Item 4.1 - ssh pi@192.168.0.12 (get IP address from master node first)
           Remove existing host name in known_hosts if necessary
           ssh-keygen -R 192.168.0.12
Item 4.2 - Enter default password - raspberry
Item 4.3 - Change master node configuration
Item 4.4 - sudo raspi-config
Item 4.5 - Change password xxyyzzqq
Item 4.6 - Change hostname: m1
Item 4.7 - Enable the ssh server (Interfacing Options)
Item 4.8 - Update configuration tool (Update)
Item 4.9 - Configure static IP address
           sudo nano /etc/dhcpcd.conf
           interface eth0
           static ip_address=192.168.0.13/24
           static routers=192.168.0.1
           static domain_name_servers=8.8.8.8 8.8.4.4
Item 4.10 - Disable swap:
            sudo dphys-swapfile swapoff
            sudo dphys-swapfile uninstall
            sudo update-rc.d dphys-swapfile remove
            sudo apt purge dphys-swapfile
Item 4.11 - Add kubernetes repository to package sources
            echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Item 4.12 - Update - (no upgrade)  available packages
            sudo apt update
            ( no takes to long) sudo apt upgrade
Item 4.13 - Install network tool and Kubernetes ADM client
            sudo apt install dnsutils kubeadm
Item 4.14 - Install latest Docker (may need to change this)
            curl -sSL get.docker.com | sh && sudo usermod pi -aG docker && \
            newgrp docker
Item 4.15 - Configure ip tables to open all ports
            (currently required for pod DNS to work):

            sudo iptables -P FORWARD ACCEPT

            Without this step, you may find coredns pods fail to start
            on app nodes and/or DNS does not work.
Item 4.16 - Configure iptables to run in legacy, since kube-proxy has
            issues with iptables version 1.8:
     
            sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

Item 4.17 - Reboot mpi1
            sudo reboot

###############################################################################
##### Item 5 - Configure Worker Node 1 - w1
###############################################################################
Item 5.1 - ssh pi@192.168.0.14 (get IP address of worker node first)
           Remove existing host name in known_hosts if necessary
           ssh-keygen -R 192.168.0.14/24
Item 5.2 - Enter default password - raspberry
Item 5.3 - Change worker node 1 configuration
Item 5.4 - sudo raspi-config
Item 5.5 - Change password xxyyzzqq
Item 5.6 - Change hostname: w1
Item 5.7 - Enable the ssh server (Interfacing Options)
Item 5.8 - Update configuration tool (Update)
Item 5.9 - Configure static IP address
           sudo nano /etc/dhcpcd.conf
           interface eth0
           static ip_address=192.168.0.14/24
           static routers=192.168.0.1
           static domain_name_servers=8.8.8.8 1.1.1.1
Item 5.10 - Disable swap:
            sudo dphys-swapfile swapoff
            sudo dphys-swapfile uninstall
            sudo update-rc.d dphys-swapfile remove
            sudo apt purge dphys-swapfile
Item 5.11 - Add kubernetes repository to package sources
            echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Item 5.12 - Update - (no upgrade)  available packages
            sudo apt update
            ( no takes to long) sudo apt upgrade
Item 5.13 - Install network tool and Kubernetes ADM client
            sudo apt install dnsutils kubeadm
Item 5.14 - Install latest Docker (may need to change this)
            curl -sSL get.docker.com | sh && sudo usermod pi -aG docker && \
            newgrp docker
Item 5.15 - Configure ip tables to open all ports
            (currently required for pod DNS to work):

            sudo iptables -P FORWARD ACCEPT

            Without this step, you may find coredns pods fail to start
            on app nodes and/or DNS does not work.
Item 5.16 - Configure iptables to run in legacy, since kube-proxy has
            issues with iptables version 1.8:

            sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

Item 5.17 - Reboot w1
            sudo reboot        

###############################################################################
##### Item 6 - Configure Worker Node 2 - w2
###############################################################################
Item 6.1 - ssh pi@192.168.0.13 (get IP address of worker node first)
           Remove existing host name in known_hosts if necessary
           ssh-keygen -R 192.168.0.13
Item 6.2 - Enter default password - raspberry
Item 6.3 - Change worker node 2 configuration
Item 6.4 - sudo raspi-config
Item 6.5 - Change password xxyyzzqq
Item 6.6 - Change hostname: w2
Item 6.7 - Enable the ssh server (Interfacing Options)
Item 6.8 - Update configuration tool (Update)
                (do not   sudo reboot)
Item 6.9 - Configure static IP address
           sudo nano /etc/dhcpcd.conf
           interface eth0
           static ip_address=192.168.0.13/24
           static routers=192.168.0.1
           static domain_name_servers=8.8.8.8 1.1.1.1
Item 6.10 - Disable swap:
            sudo dphys-swapfile swapoff
            sudo dphys-swapfile uninstall
            sudo update-rc.d dphys-swapfile remove
            sudo apt purge dphys-swapfile
Item 6.11 - Add kubernetes repository to package sources
            echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Item 6.12 - Update - (no upgrade)  available packages
            sudo apt update
            ( no takes to long) sudo apt upgrade
Item 6.13 - Install network tool and Kubernetes ADM client
            sudo apt install dnsutils kubeadm
Item 6.14 - Install latest Docker (may need to change this)
            curl -sSL get.docker.com | sh && sudo usermod pi -aG docker && \
            newgrp docker
Item 6.15 - Configure IP tables to open all ports
            (currently required for pod DNS to work):

            sudo iptables -P FORWARD ACCEPT

            Without this step, you may find coredns pods fail to start
            on app nodes and/or DNS does not work.
Item 6.16 - Configure iptables to run in legacy, since kube-proxy has
            issues with iptables version 1.8:

            sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

Item 6.17 - Reboot w2
            sudo reboot       

###############################################################################
##### Item 7 - Configure Worker Node 3 - w3
###############################################################################
Item 7.1 - ssh pi@192.168.0.16 (get IP address of worker node first)
                       Remove existing host name in known_hosts if necessary
                       ssh-keygen -R 192.168.0.16
Item 7.2 - Enter default password - raspberry
Item 7.3 - Change worker node 3 configuration
Item 7.4 - sudo raspi-config
Item 7.5 - Change password xxyyzzqq
Item 7.6 - Change hostname: w3
Item 7.7 - Enable the ssh server (Interfacing Options)
Item 7.8 - Update configuration tool (Update)
           (do not   sudo reboot)
Item 7.9 - Configure static IP address
           sudo nano /etc/dhcpcd.conf
           interface eth0
           static ip_address=192.168.0.16/24
           static routers=192.168.0.1
           static domain_name_servers=8.8.8.8 1.1.1.1
Item 7.10 - Disable swap:
            sudo dphys-swapfile swapoff
            sudo dphys-swapfile uninstall
            sudo update-rc.d dphys-swapfile remove
            sudo apt purge dphys-swapfile
Item 7.11 - Add kubernetes repository to package sources
            echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Item 7.12 - Update - (no upgrade)  available packages
            sudo apt update
            (no takes to long) sudo apt upgrade
Item 7.13 - Install network tool and Kubernetes ADM client
            sudo apt install dnsutils kubeadm
Item 7.14 - Install latest Docker (may need to change this)
            curl -sSL get.docker.com | sh && sudo usermod pi -aG docker && \
            newgrp docker
Item 7.15 - Configure IP tables to open all ports
            (currently required for pod DNS to work):

            sudo iptables -P FORWARD ACCEPT

            Without this step, you may find coredns pods fail to start
            on app nodes and/or DNS does not work.
Item 7.16 - Configure iptables to run in legacy, since kube-proxy has
            issues with iptables version 1.8:

            sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

Item 7.17 - Reboot w3
            sudo reboot                   

###############################################################################
##### Item 8 - Check Node Configuration
###############################################################################
Item 8.1 - List hosts on 192.168.0.1/24 network
           nmap -sn 192.168.0.1/24
Item 8.2 - Login to master node 1 - 192.168.0.12
           ssh pi@192.168.0.12/24
           exit
Item 8.3 - Login to worker node 1 - 192.168.0.14 
           ssh pi@192.168.0.14
           exit
Item 8.4 - Login to worker node 2 - 192.168.0.13 
           ssh pi@192.168.0.13 
           exit
Item 8.5 - Login to worker node 3 - 192.168.0.16 
           ssh pi@192.168.0.16
           exit       

###############################################################################
##### Item 9 - Set Up Control Plane
###############################################################################
Item 9.1 - Login to master node 1
           ssh pi@192.168.0.12 
Item 9.2 - Initialize cluster with defined CIDR - network range for pods along
           with a non-expiring token to enable the slaves to join the cluster.

          sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --service-cidr=10.244.240.0/20 --token-ttl=0

          W0110 21:13:45.199547   24808 validation.go:28] Cannot validate kube-proxy config - no validator is available
          W0110 21:13:45.199831   24808 validation.go:28] Cannot validate kubelet config - no validator is available
          [init] Using Kubernetes version: v1.17.0
          [preflight] Running pre-flight checks
	      [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
          [preflight] Pulling images required for setting up a Kubernetes cluster
          [preflight] This might take a minute or two, depending on the speed of your internet connection
          [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
          [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
          [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
          [kubelet-start] Starting the kubelet
          [certs] Using certificateDir folder "/etc/kubernetes/pki"
          [certs] Generating "ca" certificate and key
          [certs] Generating "apiserver" certificate and key
          [certs] apiserver serving cert is signed for DNS names [m1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.244.240.1 192.168.0.12]
          [certs] Generating "apiserver-kubelet-client" certificate and key
          [certs] Generating "front-proxy-ca" certificate and key
          [certs] Generating "front-proxy-client" certificate and key
          [certs] Generating "etcd/ca" certificate and key
          [certs] Generating "etcd/server" certificate and key
          [certs] etcd/server serving cert is signed for DNS names [m1 localhost] and IPs [192.168.0.12 127.0.0.1 ::1]
          [certs] Generating "etcd/peer" certificate and key
          [certs] etcd/peer serving cert is signed for DNS names [m1 localhost] and IPs [192.168.0.12 127.0.0.1 ::1]
          [certs] Generating "etcd/healthcheck-client" certificate and key
          [certs] Generating "apiserver-etcd-client" certificate and key
          [certs] Generating "sa" key and public key
          [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
          [kubeconfig] Writing "admin.conf" kubeconfig file
          [kubeconfig] Writing "kubelet.conf" kubeconfig file
          [kubeconfig] Writing "controller-manager.conf" kubeconfig file
          [kubeconfig] Writing "scheduler.conf" kubeconfig file
          [control-plane] Using manifest folder "/etc/kubernetes/manifests"
          [control-plane] Creating static Pod manifest for "kube-apiserver"
          [control-plane] Creating static Pod manifest for "kube-controller-manager"
          W0110 21:30:34.230236   24808 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
          [control-plane] Creating static Pod manifest for "kube-scheduler"
          W0110 21:30:34.234940   24808 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
          [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
          [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
          [apiclient] All control plane components are healthy after 32.006439 seconds
          [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
          [kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
          [upload-certs] Skipping phase. Please see --upload-certs
          [mark-control-plane] Marking the node m1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
          [mark-control-plane] Marking the node m1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
          [bootstrap-token] Using token: vspbjj.c4wqp3gph4eabd4k
          [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
          [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
          [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
          [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
          [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
          [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
          [addons] Applied essential addon: CoreDNS
          [addons] Applied essential addon: kube-proxy

          Your Kubernetes control-plane has initialized successfully!

          To start using your cluster, you need to run the following as a regular user:

          mkdir -p $HOME/.kube
          sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
          sudo chown $(id -u):$(id -g) $HOME/.kube/config

          You should now deploy a pod network to the cluster.
          Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
          https://kubernetes.io/docs/concepts/cluster-administration/addons
          kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

          Then you can join any number of worker nodes by running the following on each as root:

          kubeadm join 192.168.0.12:6443 --token vspbjj.c4wqp3gph4eabd4k \
          --discovery-token-ca-cert-hash sha256:9ccaaeb13f2d1cbdaa22df14acb83bc492c9af037d0b41060eadf4ff4924e56e 
Item 9.3 - Start Cluster
      
   Item 9.3.1   mkdir -p $HOME/.kube
   Item 9.3.2   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/confi
   Item 9.3.3   sudo chown $(id -u):$(id -g) $HOME/.kube/config
   Item 9.3.4   kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
        podsecuritypolicy.policy/psp.flannel.unprivileged created
        clusterrole.rbac.authorization.k8s.io/flannel created
        clusterrolebinding.rbac.authorization.k8s.io/flannel created
        serviceaccount/flannel created
        configmap/kube-flannel-cfg created
        daemonset.apps/kube-flannel-ds-amd64 created
        daemonset.apps/kube-flannel-ds-arm64 created
        daemonset.apps/kube-flannel-ds-arm created
        daemonset.apps/kube-flannel-ds-ppc64le created
        daemonset.apps/kube-flannel-ds-s390x created

###############################################################################
##### Item 10 - Cluster Join And Build Steps
###############################################################################

Item 10.1 - Join w1 node to the Network
    ssh pi@192.168.0.14 
    sudo kubeadm join 192.168.0.12:6443 --token vspbjj.c4wqp3gph4eabd4k \
          --discovery-token-ca-cert-hash sha256:9ccaaeb13f2d1cbdaa22df14acb83bc492c9af037d0b41060eadf4ff4924e56e 


   ##### output
   W0112 05:42:22.101491   25762 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
   [preflight] Running pre-flight checks
   [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
   [preflight] Reading configuration from the cluster...
   [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
   [kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
   [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
   [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
   [kubelet-start] Starting the kubelet
   [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

   This node has joined the cluster:
   * Certificate signing request was sent to apiserver and a response was received.
   * The Kubelet was informed of the new secure connection details.

   Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


Item 10.2 - Join w2 node to the Network
      ssh pi@192.168.0.13 
      sudo kubeadm join 192.168.0.12:6443 --token vspbjj.c4wqp3gph4eabd4k \
          --discovery-token-ca-cert-hash sha256:9ccaaeb13f2d1cbdaa22df14acb83bc492c9af037d0b41060eadf4ff4924e56e 
     
     ##### Output
     W0112 05:55:38.004926   27553 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
    [preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
    [preflight] Reading configuration from the cluster...
    [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
    [kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
    [kubelet-start] Starting the kubelet
    [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

    This node has joined the cluster:
    * Certificate signing request was sent to apiserver and a response was received.
    * The Kubelet was informed of the new secure connection details.

    Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


Item 10.3 - Join w3 node to the Network
    ssh pi@192.168.0.16 
    sudo  kubeadm join 192.168.0.12:6443 --token vspbjj.c4wqp3gph4eabd4k \
          --discovery-token-ca-cert-hash sha256:9ccaaeb13f2d1cbdaa22df14acb83bc492c9af037d0b41060eadf4ff4924e56e 

    ##### Output  
    W0112 06:00:10.968512    1621 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
    [preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
    [preflight] Reading configuration from the cluster...
    [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
    [kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
    [kubelet-start] Starting the kubelet
    [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

    This node has joined the cluster:
    * Certificate signing request was sent to apiserver and a response was received.
    * The Kubelet was informed of the new secure connection details.

    Run 'kubectl get nodes' on the control-plane to see this node join the cluster
    
    ##### Output 
    NAME   STATUS   ROLES    AGE     VERSION
    m1     Ready    master   32h     v1.17.0
    w1     Ready    <none>   19m     v1.17.0
    w2     Ready    <none>   6m15s   v1.17.0
    w3     Ready    <none>   101s    v1.17.0

