###############################################################################
#####  Raspberry Pi 4 Cluster Build [T4]
###############################################################################
##### Using code based on the following sites: 
##### https://limpygnome.com/2019/09/21/raspberry-pi-kubernetes-cluster/
#####
##### https://github.com/teamserverless/k8s-on-raspbian/blob/master/GUIDE.md
#####
##### https://blog.alexellis.io/serverless-kubernetes-on-raspberry-pi
##############################################################################

###############################################################################
##### Item 1 - Flash microSD cards with Linux OS
###############################################################################
Item 1.1 - Use Mac with etcher to load Raspbian Lite on microSD card.
Item 1.2 - 2019-09-26-raspbian-buster-lite.img
Item 1.3 - Create file named ssh in Volumes/boot
Item 1.3.1 - touch Volumes/boot/ssh

###############################################################################
##### Item 2 - Four microSD Cards, One Master And Three Workers
###############################################################################

###############################################################################
##### Item 3 - Insert microSD Cards And Power Up Network
###############################################################################
Item 3.1 - Use an Ethernet switch to connect the compute nodes
Item 3.2 - Use a different Linux or Mac computer to ssh into the node
Item 3.3 - Use namp -sP 192.168.0.0/24 get local LAN ip addresses

###############################################################################
##### Item 4 - Configure Base Master Node - m1
###############################################################################
Item 4.1 - ssh pi@192.168.0.12 (get IP address from master node first)
           Remove existing host name in known_hosts if necessary
           ssh-keygen -R 192.168.0.12
Item 4.2 - Enter default password - raspberry
Item 4.3 - Change master node configuration
Item 4.4 - sudo raspi-config
Item 4.5 - Change password xxyyzzqq
Item 4.6 - Change hostname: m1
Item 4.7 - Enable the ssh server (Interfacing Options)
Item 4.8 - Update configuration tool (Update)
Item 4.9 - Configure static IP address
           sudo nano /etc/dhcpcd.conf
           interface eth0
           static ip_address=192.168.0.13/24
           static routers=192.168.0.1
           static domain_name_servers=8.8.8.8 8.8.4.4
Item 4.10 - Disable swap:
            sudo dphys-swapfile swapoff
            sudo dphys-swapfile uninstall
            sudo update-rc.d dphys-swapfile remove
            sudo apt purge dphys-swapfile
Item 4.11 - Add kubernetes repository to package sources
            echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Item 4.12 - Update - (no upgrade)  available packages
            sudo apt update
            ( no takes to long) sudo apt upgrade
Item 4.13 - Install network tool and Kubernetes ADM client
            sudo apt install dnsutils kubeadm
Item 4.14 - Install latest Docker (may need to change this)
            curl -sSL get.docker.com | sh && sudo usermod pi -aG docker && \
            newgrp docker
Item 4.15 - Configure ip tables to open all ports
            (currently required for pod DNS to work):

            sudo iptables -P FORWARD ACCEPT

            Without this step, you may find coredns pods fail to start
            on app nodes and/or DNS does not work.
Item 4.16 - Configure iptables to run in legacy, since kube-proxy has
            issues with iptables version 1.8:
     
            sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

Item 4.17 - Reboot mpi1
            sudo reboot

###############################################################################
##### Item 5 - Configure Worker Node 1 - w1
###############################################################################
Item 5.1 - ssh pi@192.168.0.14 (get IP address of worker node first)
           Remove existing host name in known_hosts if necessary
           ssh-keygen -R 192.168.0.14/24
Item 5.2 - Enter default password - raspberry
Item 5.3 - Change worker node 1 configuration
Item 5.4 - sudo raspi-config
Item 5.5 - Change password xxyyzzqq
Item 5.6 - Change hostname: w1
Item 5.7 - Enable the ssh server (Interfacing Options)
Item 5.8 - Update configuration tool (Update)
Item 5.9 - Configure static IP address
           sudo nano /etc/dhcpcd.conf
           interface eth0
           static ip_address=192.168.0.14/24
           static routers=192.168.0.1
           static domain_name_servers=8.8.8.8 1.1.1.1
Item 5.10 - Disable swap:
            sudo dphys-swapfile swapoff
            sudo dphys-swapfile uninstall
            sudo update-rc.d dphys-swapfile remove
            sudo apt purge dphys-swapfile
Item 5.11 - Add kubernetes repository to package sources
            echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Item 5.12 - Update - (no upgrade)  available packages
            sudo apt update
            ( no takes to long) sudo apt upgrade
Item 5.13 - Install network tool and Kubernetes ADM client
            sudo apt install dnsutils kubeadm
Item 5.14 - Install latest Docker (may need to change this)
            curl -sSL get.docker.com | sh && sudo usermod pi -aG docker && \
            newgrp docker
Item 5.15 - Configure ip tables to open all ports
            (currently required for pod DNS to work):

            sudo iptables -P FORWARD ACCEPT

            Without this step, you may find coredns pods fail to start
            on app nodes and/or DNS does not work.
Item 5.16 - Configure iptables to run in legacy, since kube-proxy has
            issues with iptables version 1.8:

            sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

Item 5.17 - Reboot w1
            sudo reboot        

###############################################################################
##### Item 6 - Configure Worker Node 2 - w2
###############################################################################
Item 6.1 - ssh pi@192.168.0.13 (get IP address of worker node first)
           Remove existing host name in known_hosts if necessary
           ssh-keygen -R 192.168.0.13
Item 6.2 - Enter default password - raspberry
Item 6.3 - Change worker node 2 configuration
Item 6.4 - sudo raspi-config
Item 6.5 - Change password xxyyzzqq
Item 6.6 - Change hostname: w2
Item 6.7 - Enable the ssh server (Interfacing Options)
Item 6.8 - Update configuration tool (Update)
                (do not   sudo reboot)
Item 6.9 - Configure static IP address
           sudo nano /etc/dhcpcd.conf
           interface eth0
           static ip_address=192.168.0.13/24
           static routers=192.168.0.1
           static domain_name_servers=8.8.8.8 1.1.1.1
Item 6.10 - Disable swap:
            sudo dphys-swapfile swapoff
            sudo dphys-swapfile uninstall
            sudo update-rc.d dphys-swapfile remove
            sudo apt purge dphys-swapfile
Item 6.11 - Add kubernetes repository to package sources
            echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Item 6.12 - Update - (no upgrade)  available packages
            sudo apt update
            ( no takes to long) sudo apt upgrade
Item 6.13 - Install network tool and Kubernetes ADM client
            sudo apt install dnsutils kubeadm
Item 6.14 - Install latest Docker (may need to change this)
            curl -sSL get.docker.com | sh && sudo usermod pi -aG docker && \
            newgrp docker
Item 6.15 - Configure IP tables to open all ports
            (currently required for pod DNS to work):

            sudo iptables -P FORWARD ACCEPT

            Without this step, you may find coredns pods fail to start
            on app nodes and/or DNS does not work.
Item 6.16 - Configure iptables to run in legacy, since kube-proxy has
            issues with iptables version 1.8:

            sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

Item 6.17 - Reboot w2
            sudo reboot       

###############################################################################
##### Item 7 - Configure Worker Node 3 - w3
###############################################################################
Item 7.1 - ssh pi@192.168.0.16 (get IP address of worker node first)
                       Remove existing host name in known_hosts if necessary
                       ssh-keygen -R 192.168.0.16
Item 7.2 - Enter default password - raspberry
Item 7.3 - Change worker node 3 configuration
Item 7.4 - sudo raspi-config
Item 7.5 - Change password xxyyzzqq
Item 7.6 - Change hostname: w3
Item 7.7 - Enable the ssh server (Interfacing Options)
Item 7.8 - Update configuration tool (Update)
           (do not   sudo reboot)
Item 7.9 - Configure static IP address
           sudo nano /etc/dhcpcd.conf
           interface eth0
           static ip_address=192.168.0.16/24
           static routers=192.168.0.1
           static domain_name_servers=8.8.8.8 1.1.1.1
Item 7.10 - Disable swap:
            sudo dphys-swapfile swapoff
            sudo dphys-swapfile uninstall
            sudo update-rc.d dphys-swapfile remove
            sudo apt purge dphys-swapfile
Item 7.11 - Add kubernetes repository to package sources
            echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Item 7.12 - Update - (no upgrade)  available packages
            sudo apt update
            (no takes to long) sudo apt upgrade
Item 7.13 - Install network tool and Kubernetes ADM client
            sudo apt install dnsutils kubeadm
Item 7.14 - Install latest Docker (may need to change this)
            curl -sSL get.docker.com | sh && sudo usermod pi -aG docker && \
            newgrp docker
Item 7.15 - Configure IP tables to open all ports
            (currently required for pod DNS to work):

            sudo iptables -P FORWARD ACCEPT

            Without this step, you may find coredns pods fail to start
            on app nodes and/or DNS does not work.
Item 7.16 - Configure iptables to run in legacy, since kube-proxy has
            issues with iptables version 1.8:

            sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

Item 7.17 - Reboot w3
            sudo reboot                   

###############################################################################
##### Item 8 - Check Node Configuration
###############################################################################
Item 8.1 - List hosts on 192.168.0.1/24 network
           nmap -sn 192.168.0.1/24
Item 8.2 - Login to master node 1 - 192.168.0.12
           ssh pi@192.168.0.12/24
           exit
Item 8.3 - Login to worker node 1 - 192.168.0.14 
           ssh pi@192.168.0.14
           exit
Item 8.4 - Login to worker node 2 - 192.168.0.13 
           ssh pi@192.168.0.13 
           exit
Item 8.5 - Login to worker node 3 - 192.168.0.16 
           ssh pi@192.168.0.16
           exit       

###############################################################################
##### Item 9 - Set Up Control Plane
###############################################################################
Item 9.1 - Login to master node 1
           ssh pi@192.168.0.12 
Item 9.2 - Initialize cluster with defined CIDR - network range for pods along
           with a non-expiring token to enable the slaves to join the cluster.

          sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --service-cidr=10.244.240.0/20 --token-ttl=0

          W0110 21:13:45.199547   24808 validation.go:28] Cannot validate kube-proxy config - no validator is available
          W0110 21:13:45.199831   24808 validation.go:28] Cannot validate kubelet config - no validator is available
          [init] Using Kubernetes version: v1.17.0
          [preflight] Running pre-flight checks
	      [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
          [preflight] Pulling images required for setting up a Kubernetes cluster
          [preflight] This might take a minute or two, depending on the speed of your internet connection
          [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
          [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
          [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
          [kubelet-start] Starting the kubelet
          [certs] Using certificateDir folder "/etc/kubernetes/pki"
          [certs] Generating "ca" certificate and key
          [certs] Generating "apiserver" certificate and key
          [certs] apiserver serving cert is signed for DNS names [m1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.244.240.1 192.168.0.12]
          [certs] Generating "apiserver-kubelet-client" certificate and key
          [certs] Generating "front-proxy-ca" certificate and key
          [certs] Generating "front-proxy-client" certificate and key
          [certs] Generating "etcd/ca" certificate and key
          [certs] Generating "etcd/server" certificate and key
          [certs] etcd/server serving cert is signed for DNS names [m1 localhost] and IPs [192.168.0.12 127.0.0.1 ::1]
          [certs] Generating "etcd/peer" certificate and key
          [certs] etcd/peer serving cert is signed for DNS names [m1 localhost] and IPs [192.168.0.12 127.0.0.1 ::1]
          [certs] Generating "etcd/healthcheck-client" certificate and key
          [certs] Generating "apiserver-etcd-client" certificate and key
          [certs] Generating "sa" key and public key
          [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
          [kubeconfig] Writing "admin.conf" kubeconfig file
          [kubeconfig] Writing "kubelet.conf" kubeconfig file
          [kubeconfig] Writing "controller-manager.conf" kubeconfig file
          [kubeconfig] Writing "scheduler.conf" kubeconfig file
          [control-plane] Using manifest folder "/etc/kubernetes/manifests"
          [control-plane] Creating static Pod manifest for "kube-apiserver"
          [control-plane] Creating static Pod manifest for "kube-controller-manager"
          W0110 21:30:34.230236   24808 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
          [control-plane] Creating static Pod manifest for "kube-scheduler"
          W0110 21:30:34.234940   24808 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
          [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
          [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
          [apiclient] All control plane components are healthy after 32.006439 seconds
          [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
          [kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
          [upload-certs] Skipping phase. Please see --upload-certs
          [mark-control-plane] Marking the node m1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
          [mark-control-plane] Marking the node m1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
          [bootstrap-token] Using token: vspbjj.c4wqp3gph4eabd4k
          [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
          [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
          [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
          [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
          [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
          [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
          [addons] Applied essential addon: CoreDNS
          [addons] Applied essential addon: kube-proxy

          Your Kubernetes control-plane has initialized successfully!

          To start using your cluster, you need to run the following as a regular user:

          mkdir -p $HOME/.kube
          sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
          sudo chown $(id -u):$(id -g) $HOME/.kube/config

          You should now deploy a pod network to the cluster.
          Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
          https://kubernetes.io/docs/concepts/cluster-administration/addons
          kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

          Then you can join any number of worker nodes by running the following on each as root:

          kubeadm join 192.168.0.12:6443 --token vspbjj.c4wqp3gph4eabd4k \
          --discovery-token-ca-cert-hash sha256:9ccaaeb13f2d1cbdaa22df14acb83bc492c9af037d0b41060eadf4ff4924e56e 
Item 9.3 - Start Cluster
      
   Item 9.3.1   mkdir -p $HOME/.kube
   Item 9.3.2   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/confi
   Item 9.3.3   sudo chown $(id -u):$(id -g) $HOME/.kube/config
   Item 9.3.4   kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
        podsecuritypolicy.policy/psp.flannel.unprivileged created
        clusterrole.rbac.authorization.k8s.io/flannel created
        clusterrolebinding.rbac.authorization.k8s.io/flannel created
        serviceaccount/flannel created
        configmap/kube-flannel-cfg created
        daemonset.apps/kube-flannel-ds-amd64 created
        daemonset.apps/kube-flannel-ds-arm64 created
        daemonset.apps/kube-flannel-ds-arm created
        daemonset.apps/kube-flannel-ds-ppc64le created
        daemonset.apps/kube-flannel-ds-s390x created

###############################################################################
##### Item 10 - Cluster Join And Build Steps
###############################################################################

Item 10.1 - Join w1 node to the Network
    ssh pi@192.168.0.14 
    sudo kubeadm join 192.168.0.12:6443 --token vspbjj.c4wqp3gph4eabd4k \
          --discovery-token-ca-cert-hash sha256:9ccaaeb13f2d1cbdaa22df14acb83bc492c9af037d0b41060eadf4ff4924e56e 


   ##### Output
   W0112 05:42:22.101491   25762 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
   [preflight] Running pre-flight checks
   [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
   [preflight] Reading configuration from the cluster...
   [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
   [kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
   [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
   [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
   [kubelet-start] Starting the kubelet
   [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

   This node has joined the cluster:
   * Certificate signing request was sent to apiserver and a response was received.
   * The Kubelet was informed of the new secure connection details.

   Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


Item 10.2 - Join w2 node to the Network
      ssh pi@192.168.0.13 
      sudo kubeadm join 192.168.0.12:6443 --token vspbjj.c4wqp3gph4eabd4k \
          --discovery-token-ca-cert-hash sha256:9ccaaeb13f2d1cbdaa22df14acb83bc492c9af037d0b41060eadf4ff4924e56e 
     
     ##### Output
     W0112 05:55:38.004926   27553 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
    [preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
    [preflight] Reading configuration from the cluster...
    [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
    [kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
    [kubelet-start] Starting the kubelet
    [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

    This node has joined the cluster:
    * Certificate signing request was sent to apiserver and a response was received.
    * The Kubelet was informed of the new secure connection details.

    Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


Item 10.3 - Join w3 node to the Network
    ssh pi@192.168.0.16 
    sudo  kubeadm join 192.168.0.12:6443 --token vspbjj.c4wqp3gph4eabd4k \
          --discovery-token-ca-cert-hash sha256:9ccaaeb13f2d1cbdaa22df14acb83bc492c9af037d0b41060eadf4ff4924e56e 

    ##### Output  
    W0112 06:00:10.968512    1621 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
    [preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
    [preflight] Reading configuration from the cluster...
    [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
    [kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
    [kubelet-start] Starting the kubelet
    [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

    This node has joined the cluster:
    * Certificate signing request was sent to apiserver and a response was received.
    * The Kubelet was informed of the new secure connection details.

    Run 'kubectl get nodes' on the control-plane to see this node join the cluster
    
    ##### Output 
    NAME   STATUS   ROLES    AGE     VERSION
    m1     Ready    master   32h     v1.17.0
    w1     Ready    <none>   19m     v1.17.0
    w2     Ready    <none>   6m15s   v1.17.0
    w3     Ready    <none>   101s    v1.17.
    
###############################################################################
##### Item 11 - Install Kubernetes Dashboard
###############################################################################
Item 11.1 - Login to master nod - m1 
     ssh pi@192.168.0.12 
Item 11.2 - Install the Dashboard
     kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml

     ##### Output
     namespace/kubernetes-dashboard created
     serviceaccount/kubernetes-dashboard created
     service/kubernetes-dashboard created
     secret/kubernetes-dashboard-certs created
     secret/kubernetes-dashboard-csrf created
     secret/kubernetes-dashboard-key-holder created
     configmap/kubernetes-dashboard-settings created
     role.rbac.authorization.k8s.io/kubernetes-dashboard created
     clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
     rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
     clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
     deployment.apps/kubernetes-dashboard created
     service/dashboard-metrics-scraper created
     deployment.apps/dashboard-metrics-scraper created


Item 11.3 - Create user.yaml file
     nano user.yaml

     ##### File contents
     apiVersion: v1
     kind: ServiceAccount
     metadata:
       name: admin-user
       namespace: kube-system
     ---
     apiVersion: rbac.authorization.k8s.io/v1
     kind: ClusterRoleBinding
     metadata:
       name: admin-user
     roleRef:
       apiGroup: rbac.authorization.k8s.io
       kind: ClusterRole
       name: cluster-admin
     subjects:
     - kind: ServiceAccount
       name: admin-user
       namespace: kube-system

Item 11.4 Apply user.yaml file to setup Dashboard
    kubectl apply -f user.yaml

    ##### Output
    serviceaccount/admin-user created
    clusterrolebinding.rbac.authorization.k8s.io/admin-user created


###############################################################################
##### Item 12 - Access Kubernetes Dashboard
###############################################################################
Item 12.1 - Create dashboard shell script on local linux machne (not in cluster)
    nano dashboard.sh

    That contains:
    #####
    MASTER="pi@192.168.0.12"

    # Print token for login
    TOKEN_COMMAND="kubectl -n kube-system describe secret \$(kubectl -n kube-system get secret | grep admin-user | awk '{print \$1}')"

    echo "Dumping token for dashboard..."
    ssh ${MASTER} -C "${TOKEN_COMMAND}"

    echo "Login:"
    echo "  http://localhost:8089/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login"

    # Create SSH tunnel to k8 master and run proxy
    echo "Creating proxy tunnel to this machine from master..."
    ssh -L 8089:localhost:8089 ${MASTER} -C "kubectl proxy --port=8089 || true"

    echo "Terminate proxy..."
    ssh ${MASTER} -C "pkill kubectl"

Item 12.2 Change mode of and run dashboard.sh file
     chmod 755 dashboard.sh
     ,/dashboard.sh

     ##### Output

     Dumping token for dashboard...
pi@192.168.0.12's password: 
Name:         admin-user-token-hjc7j
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 059ba9e5-e180-4ba0-a639-7024c73d4324

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjVEX3NVQlZFWjhpWUxpSHVZUFRYY1BDLWcyMGNfc0h5bmdMb0s1dG5aSGsifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWhqYzdqIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwNTliYTllNS1lMTgwLTRiYTAtYTYzOS03MDI0YzczZDQzMjQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.xPvmU3pd6autDgc6irCHmCxNQyGwZKmr-q7b498yQ2jx5O4TLRv8ML9ygdyt3E1sZCa-fa7e5fQTmOgu3t5YUHP3mthTIp4t_aQWkzaOUOqin-PPvGNZhxSY_TxuPeW4o-CI6exsn9LKC6EhuFB5AyOU650oTDCu_7oNGa27JnqakzV9ZbWyVc335zKzdpaFyobywQyKTQSUgGv4DizyA24O4_KwatOXI8bxbNsrxkxDQQlMXm6z4dfyP9CmD9vePKdn60GxHPGb9xJlAojt0MPMzqpHcSTWbmCWrFDlTzkfp6diVBytAqbTp5kahDcVrY-XahAg8cqLy2qxWlMz9A
Login:
  http://localhost:8089/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login
Creating proxy tunnel to this machine from master...
pi@192.168.0.12's password: 
Starting to serve on 127.0.0.1:8089


###############################################################################
##### Item 13 - Deploy Nginx On The Cluster
##############################################################################
## From Master Node - m1 -

kubectl create deployments nginx --image=ngix

###############################################################################
##### Item 14 - Install Helm
##############################################################################
## On Master Node - m1 

 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
 chmod 700 get_helm.sh
 ./get_helm.sh

 ##### Output 
 Downloading https://get.helm.sh/helm-v3.0.2-linux-arm.tar.gz
Preparing to install helm into /usr/local/bin
helm installed into /usr/local/bin/helm

###############################################################################
##### Item 15 - Install Nginx Using Helm
##############################################################################
## On Master Node - m1

#### Add NGINX Helm repository:

helm repo add nginx-stable https://helm.nginx.com/stable

### "nginx-stable" has been added to your repositories

 helm repo update

 ### Hang tight while we grab the latest from your chart repositories...
 ### ...Successfully got an update from the "nginx-stable" chart repository
 ### Update Complete. ⎈ Happy Helming!⎈

helm install nginx-stable/nginx-ingress --generate-name

### NAME: nginx-ingress-1578894679
### LAST DEPLOYED: Mon Jan 13 05:51:20 2020
### NAMESPACE: default
### STATUS: deployed
### REVISION: 1
### TEST SUITE: None
### NOTES:
### The NGINX Ingress Controller has been installed.

### remove nginx-ingress 
nginx-ingress-1578894679

